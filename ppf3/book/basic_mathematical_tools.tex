\chapter{Basic Mathematical Tools}\label{ch:basic-mathematical-tools}

There are some basic mathematical tools and algorithms that are used
constantly in computational quantitative analysis. Reviewing the
implementation of these in Python gives us a good work-out in Python
programming and the implementations provide us with needed tools to
construct more advanced programs in later chapters.

\section{Random number generation}

A module for pseudo-random number generators is provided in the Python
libraries. It uses the \emph{Mersenne Twister}\footnote{The Mersenne
Twister 19939 (often referred to as just `MT19939') is a psuedo-random
generator developed in 1977 by Makoto Matsumoto and Takuji Nishimura.}
algorithm as the core generator, one of the most extensively tested
random number generation schemes of all time. The following is a program
demonstrating how the module can be used. The program prints firstly
$100$ samples from a Gaussian distribution with mean $\mu = 0$ and
standard deviation $\sigma=1$ and then $100$ samples from a lognormal
distribution with the same $\mu$ and $\sigma$.
\begin{verbatim}
import random, sys, getopt

def _print_gauss():
  g = random.Random(1234)
  print [g.gauss(mu = 0, sigma = 1) for i in range(100)]

def _print_lognormal_variate():
  g = random.Random(1234)
  print [g.lognormvariate(mu = 0, sigma = 1) for i in range(100)]

def _usage():
  print "usage: %s" % sys.argv[0]
  print "Try `python %s -h' for more information." % sys.argv[0]

def _help():
  print "usage: %s" % sys.argv[0]
  print "-h (--help)            : print this help message and exit"
  print "-v (--version)         : print the version number and exit"

if __name__ == '__main__':
  try:
   opts, args, = getopt.getopt(sys.argv[1:], "vh", ["version", "help", ])
  except getopt.GetoptError:
    _usage()
    sys.exit(2)
  for o, a in opts:
    if o in ("-h", "--help"):
      _help()
      sys.exit()
    if o in ("-v", "--version"):
      print "'%s', Version 0.0.0" % sys.argv[0]
      sys.exit()
  _print_gauss()
  _print_lognormal_variate()
\end{verbatim}

\section{$N(.)$}
In the \verb|ppf.math.special_functions| module, \verb|N| is a
function that approximates the standard normal cumulative distribution
function, $N(.)$, as used in the celebrated Black-Scholes option
pricing equation.
\begin{verbatim}
import math

def N(x):
  a   =  0.3535533905933
  b1  = -1.2655122300000
  b2  =  1.0000236800000
  b3  =  0.3740919600000
  b4  =  0.0967841800000
  b5  = -0.1862880600000
  b6  =  0.2788680700000
  b7  = -1.1352039800000
  b8  =  1.4885158700000
  b9  = -0.8221522300000
  b10 =  0.1708727700000

  t, term, result = 0, 0, 0
            
  if(x > 0):
    if (x > 10): result = 1.0
    else:
      t = 1/(1 + a*x)
      term = b9 + t*b10
      term = b8 + t*term
      term = b7 + t*term
      term = b6 + t*term
      term = b5 + t*term
      term = b4 + t*term
      term = b3 + t*term
      term = b2 + t*term
      term = b1 + t*term
      term = term + -0.5*(x*x) 

      result = 1.0 - 0.5*t*math.exp(term)
  else:
    if(x < -10): result = 0.0
    else:
      t = 1/(1 - a*x)
      term = b9 + t*b10
      term = b8 + t*term
      term = b7 + t*term
      term = b6 + t*term
      term = b5 + t*term
      term = b4 + t*term
      term = b3 + t*term
      term = b2 + t*term
      term = b1 + t*term
      term = term + -0.5*(x*x)
            
      result = 0.5*t*math.exp(term)
  
  return result
\end{verbatim}

\section{Interpolation}\label{sec:interpolation}

Interpolation is the process of estimating the values of a function
$y(x)$ for arguments between $x_{0},\ldots,x_{n}$ at which the values
$y_{0},\ldots,y_{n}$ are known. To elegantly implement interpolation
schemes in a single dimension, it is helpful to first define some
utility functions for searching an ordered sequence of numbers. The
\verb|ppf.utility.bound| module defines a family of such functions in
the spirit of the C++ STL\footnote{The C++ STL (Standard Template
Library) is a generic library of class templates and algorithms.}
functions of the same names.
\begin{verbatim}
import operator

def lower_bound(x, values, cmp=operator.lt):
  """Find the first position in values
  where x could be inserted without violating
  the ordering.
  """
  first, count = 0, len(values)
  while count > 0:
    half = count/2
    middle = first + half
    if cmp(values[middle], x):
          first = middle + 1
          count = count - half - 1
    else: count = half

  return first

def upper_bound(x, values, cmp=operator.lt):
  """Finds the last position in values
  where x could be inserted without changing
  the ordering.
  """
  first, count = 0, len(values)
  while count > 0:
    half = count/2
    middle = first + half
    if cmp(x, values[middle]):
      count = half
    else:
      first = middle + 1
      count = count - half - 1

  return first

def equal_range(x, values, cmp=operator.lt):
  """Find the largest subrange in which
    x could be inserted in any place without
    changing the ordering.
  """
  return (lower_bound(x, values, cmp), upper_bound(x, values, cmp))

def bound(x, values, cmp=operator.lt):
  """Raise if x is outside of the domain
  else find indices, i, j such that values[i] <= x <= values[j].

  """
  count = len(values)
  left, right = equal_range(x, values, cmp)
  if left == count:
    raise RuntimeError, "%f lies right of the domain" % x
  elif right == 0:
    raise RuntimeError, "%f lies left of the domain" % x

  if right == count: right -= 1
  if left == right:   left -= 1

  return (left, right)
\end{verbatim}
The classic user case for the \verb|bound| function in the context of interpolation 
is to find an index $j$ such that, given an ordered sequence of real numbers, 
$x_{1}, \ldots, x_{N}, $ $x_{j-1} \le x \le x_{j}$:
\begin{verbatim}
  def test_bound(self):
    bound = ppf.utility.bound
    values = [1, 2, 3]
    i, j = bound(1.5, values)
    assert i == j -1 and values[i] <= 1.5 <= values[j]
    i, j = bound(2.0, [1, 2, 3])
    assert i == j -1 and values[i] <= 2.0 <= values[j]
    self.assertRaises(RuntimeError, bound, 4, values)
\end{verbatim}
The parameterisation of the \verb|bound| algorithm by the user provided less-than
predicate admits other interesting uses. The following unit test shows \verb|bound|
in conjunction with case insensitive string comparison:
\begin{verbatim}
  def test_bound_ci(self):
    bound = ppf.utility.bound
    values = ['ape', 'Apple', 'caNada']
    i, j = bound('bananana', values
                 , lambda x, y: x.lower() < y.lower())
    assert i == j -1 and values[i].lower() <= 'banana' <= values[j].lower()
\end{verbatim}
With the function \verb|bound| at our disposal, implementing a variety of interpolation
schemes becomes easy. First, the \verb|ppf.math.interpolation| module defines a base
class for interpolators:
\begin{verbatim}
import math
import ppf.utility
import linear_algebra

class interpolation_base:
  def __init__(self, abscissae, ordinates):
    if not sorted(abscissae) or \
         len(abscissae) != len(ordinates):
      raise RuntimeError, \
            'abscissae/ordinates length mismatch'
    self.N = len(abscissae)
    self.abscissae, self.ordinates = abscissae, ordinates

  def locate(self, x):
    i, j = ppf.utility.bound(x, self.abscissae)
    x_lo, x_hi = self.abscissae[i], self.abscissae[j]
    y_lo, y_hi = self.ordinates[i], self.ordinates[j]

    return (i, j, x_lo, x_hi, y_lo, y_hi)
\end{verbatim}
This base class essentially wraps up the business of locating the
points in a sequence that will participate in the interpolation by 
virtue of the \verb|bound| function. With this utility in hand,
we move on to a variety of interpolation schemes.

\subsection{Linear interpolation}
In this scheme, if $x_{i-1} \leq x < x_{i}$ we estimate $y(x)$ by
\begin{equation}
  y = \left(\frac{x-x_{i-1}}{x_{i}-x_{i-1}}\right)(y_{i}-y_{i-1}) + y_{i-1}.
\end{equation}
If we define the quantity $R$ by $R = \frac{x-x_{i-1}}{x_{i}-x_{i-1}}$
, then in terms of R we find
\begin{equation}
 y = R\label{eq:linear-interpolation}\left(y_{i}-y_{i-1}\right) + y_{i-1}.
\end{equation}
So, saying this in Python code yields
\begin{verbatim}
class linear(interpolation_base):
  def __init__(self, abscissae, ordinates):
    interpolation_base.__init__(self, abscissae, ordinates)

  def __call__(self, x):
    i, j, x_lo, x_hi, y_lo, y_hi = \
       interpolation_base.locate(self, x)
    R = 1.0 - (x_hi - x)/(x_hi - x_lo)

    return R*(y_hi - y_lo) + y_lo

\end{verbatim}

\subsection{Log-linear interpolation}
In this scheme, we estimate $y(x)$ by
\begin{equation}
  y = e^{ln(y_{i-1})+\left(ln(y_{i})-ln(y_{i-1})\right)R}. \label{eq:log-linear-interpolation}
\end{equation}
%In addition to estimating $y(x)$ by equation
%\ref{eq:log-linear-interpolation} we also have that if $x = x_{i}$ for
%some $i$ then $y = y(x_{i})$ and $\frac{\partial{y}}{\partial{y_{i}}}
%= 1$. Otherwise, if $x_{i-1} < x < x_{i}$ then
%\begin{eqnarray}
%  \frac{\partial{y}}{\partial{y_{i-1}}} & = & \frac{y}{y_{i-1}}(1 - R) \\
%  \frac{\partial{y}}{\partial{y_{i}}}   & = & \frac{y}{y_{i}}R.
%\end{eqnarray}
In Python:
\begin{verbatim}
class loglinear(interpolation_base):
  def __init__(self, abscissae, ordinates):
    interpolation_base.__init__(self, abscissae, ordinates)

  def __call__(self, x):
    i, j, x_lo, x_hi, y_lo, y_hi = \
       interpolation_base.locate(self, x)
    ln_ylo, ln_yhi = math.log(y_lo), math.log(y_hi)
    R = 1.0 - (x_hi - x)/(x_hi - x_lo)

    return math.exp(ln_ylo+(ln_yhi - ln_ylo)*R)
\end{verbatim}

\subsection{Linear on zero interpolation}
In this scheme, we estimate $y(x)$ in the following way. First, if
$i-1 = 0$ then
\begin{equation}
  y = y^{\left(\frac{x-x_{0}}{x_{i}-x_{i-1}}\right)}_{i}
\end{equation}
otherwise
\begin{equation}
  y = e^{-\left(z_{i-1} +R\left(z_{i}-z_{i-1}\right)\right)\left(x-x_{0}\right)} \label{eq:loz-linear-interpolation}
\end{equation}
with
\begin{equation}
  z_{i} = \frac{-ln(y_{i})}{x_{i}-x_{0}}.
\end{equation}
%In addition to estimating $y(x)$ by equation \ref{eq:loz-linear-interpolation} we also have that if $x = x_{i}$ for some $i$ then $y = y(x_{i})$ and $\frac{\partial{y}}{\partial{y_{i}}} = 1$. Otherwise, if $x_{i-1} < x < x_{i}$ then 
%\begin{eqnarray}
%  \frac{\partial{y}}{\partial{y_{i-1}}} & = & \frac{y}{y_{i-1}}\left(\frac{x-x_{0}}{x_{i-1}-x_{0}}\right)\left(1 - R\right) \\ 
%  \frac{\partial{y}}{\partial{y_{i}}}   & = & \frac{y}{y_{i}}\left(\frac{x-x_{0}}{x_{i}-x_{0}}\right)R.
%\end{eqnarray}
Putting the above into Python code we get
\begin{verbatim}
class linear_on_zero(interpolation_base):
  def __init__(self, abscissae, ordinates):
    interpolation_base.__init__(self, abscissae, ordinates)

  def __call__(self, x):
    x_0 = self.abscissae[0]
    i, j, x_lo, x_hi, y_lo, y_hi = \
       interpolation_base.locate(self, x)
    dx = (x_hi - x_lo)
    R, R_ = (1.0 - ((x_hi - x)/dx)), (x - x_0)/dx
    y = 0
    if i == 0:
      y = math.pow(y_hi, R_)
    else:
      r, r_lo, r_hi = x - x_0, x_lo - x_0, x_hi - x_0
      z_lo, z_hi = -math.log(y_lo)/r_lo, -math.log(y_hi)/r_hi
      y = math.exp(-(z_lo + R*(z_hi - z_lo))*r)

    return y
\end{verbatim}

\subsection{Cubic spline interpolation}
Another popular interpolation method, popular because the curves it
produces are particularly smooth, is to let the fitting function be a
piecewise union of cubic polynomials. That is, we define a polynomial
$P_i$ on each interval $[a_{i-1},a_i]$ such that the endpoints of the
polynomial pass through the ordinates $y_{i}$ and that the first and
second derivatives of the cubic match with the next cubic along -
i.e.:
\begin{eqnarray*}
P_i(x_i) &=& y_i \\
P_{i-1}(x_i) &=& y_i \\
\frac{d}{dx}P_i(x_{i-1}) &=& \frac{d}{dx}P_{i-1}(x_i) \\
\frac{d^2}{dx^2}P_i(x_{i-1}) &=& \frac{d^2}{dx^2}P_{i-1}(x_i) 
\end{eqnarray*}
for all $i$. By imposing conditions on the values of the derivative at
the very endpoints of the function $x_0$ and $x_{N-1}$ there are 
sufficiently many conditions for the coefficients of all the cubics to
be determined uniquely by solving a linear system of equations. The exact
form of this linear system varies from one source to another, we use 
the form found in \cite{book:SONA}.

Given $x$, let $i$ be such that $a_{i-1} < x < a_i$.  Then our
formulation says that our cubic for this i-th segment is
\begin{eqnarray}
p(x) &=& \frac{c_{i-1}*(a_i-x)^3}{6h_i} \nonumber\\
&+& \frac{c_i(x-a_{i-1})^3}{6h_i(a_i-a_{i-1})} \nonumber\\
&+& (y_{i-1} - \frac{c_{i-1}h_i^2}{6})(\frac{a_i-x}{h_i}) \nonumber\\
&+& (y_i - \frac{c_{i}h_i^2}{6})(\frac{x-a_{i-1}}{h_i})
\end{eqnarray}
where $c$ is a set of vectors linearly dependent on the ordinates
$y_{i}$ that we will determine and $h_i$ is the width of the segment
$(=a_i-a_{i-1})$.
%Because $c$ is a linear function of the \yi and p is a linear function
%of the $c$s, it follows that the p is a linear function of the \yi
%also. Thus once we have found all the partial derivatives
%$\frac{\partial{c_i}}{\partial{y_j}}$, we may easily find
%$\frac{\partial{p} }{\partial{y_j}}$.
$c$ is determined by the linear system of equations $Ac=b$ where A is
square tridiagonal matrix whose values are dependent only on the
segment widths $h_i$ and each $b$ is a linear combination of the
$y_i$. More specifically
\begin{eqnarray}
b_0 &=& d_{left} \nonumber\\
b_i &=& \frac{6}{h_i+h_{i+1}}(\frac{y_{i+1}-y_i}{h_{i+1}} - \frac{y_i - y_{i-1}}{h_{i}}) \nonumber\\
b_n &=& d_{right}
\end{eqnarray}
where $d_{left}$ and $d_{right}$ are constants dependent only on the
choice of the value of the derivatives at the endpoints of the curve
\footnote{In the case of the so-called \emph{natural spline}, we set
the derivatives at the end-points to be zero, and have
$d_{left}=d_{right}=0.0$}.
%A is invertible, let $F = A^{-1}$. Then
%we have
%\begin{eqnarray}
%c_i &=& F_{i,0}d_{left} + F_{i,N-1}d_{right} +  6\frac{F_{i,1}}{h_1(H_1+h_1)}F_{i,1}y_0 \nonumber\\
%&+& 6\left( -\frac{F_{i,1}}{H_1(H_1+h_1)} - \frac{F_{i,1}}{h_1(H_1+h_1)} + \frac{F_{i,2}}{h_2(H_2+H_2)}\right)y_1 \nonumber\\
%&+& 6\left(\Sigma_{j=2}^{N-3}\left(\frac{F_{i,j-1}}{H_{j-1}(H_{j-1}+h_{j-1})} - \frac{F_{i,j}}{H_j(H_j+h_j)} - \frac{F_{i,j}}{h_j(H_j+h_j)} + \frac{F_{i,j+1}}{h_{j+1}(H_{j+1}+h_{j+1}}\right)\right)y_j \nonumber\\
%&+& 6\left( -\frac{F_{i,N-2}}{H_{N-2}(H_{N-2}+h_{N-2})}-\frac{F_{i,N-1}}{h_{N-2}(H_{N-2}+h_{N-2})} + \frac{F_{i,N-3}}{H_{N-3}(H_{N-3}+h_{N-3})}\right)y_{N-2} \nonumber\\
%&+& 6\frac{F_{i,N-2}}{H_{N-2}(H_{N-2}+h_{N-2})}y_{N-1} 
%\end{eqnarray}
%where $h_i=a_i-a_{i-1}$ as before and $H_i = h_{i+1}$. Thus the $c_i$ are of the form:
%\begin{equation}
%c_i = l_i + \Sigma_{j} k_{i,j}y_j
%\end{equation}
%for some known constants $k_{i,j}$ and $l_i$. This means we have
%\begin{equation}
%\frac{\partial c_i}{\partial y_j} = k_{i,j}
%\end{equation}
%Plugging this into our original formula for $p$ we have
%\begin{eqnarray}
%\frac{\partial p}{\partial y_j}(x) &=& \frac{(a_i-x)^3}{6h_i}k_{i-1,j} \nonumber\\
%&+& (\frac{x-a_{i-1})^3}{6h_i}k_{i,j} \nonumber\\
%&+& (\delta_{i-1,j} - \frac{k_{i-1,j}h_i^2}{6})\frac{x_i - x}{a_i-a_{i-1}} \nonumber\\
%&+& (\delta_{i,j} - \frac{k_{i,j}h_i^2}{6})\frac{x_i-x}{a_i-a_{i-1}}
%\end{eqnarray}
Implementation of this scheme in Python requires a little more effort than
the earlier cases:
\begin{verbatim}
class cubic_spline(interpolation_base):
  def __init__(self, abscissae, ordinates, a_0 = 0.5, d_0=0, b_n=0.5, d_n=0):
    interpolation_base.__init__(self, abscissae, ordinates)
    xs, ys, N = self.abscissae, self.ordinates, self.N
    b = [d_0]+(N - 1)*[0]
    A_sub, A_dia, A_sup = N*[0], [2.0] + (N - 1)*[0], [a_0] + (N - 1)*[0]
    for i in range(1, N - 1):
      H, h = xs[i + 1]- xs[i], xs[i] -  xs[i - 1]
      b[i] = (6./(h + H))*(((ys[i + 1] - ys[i])/H) - ((ys[i] - ys[i - 1])/h))
      a_i = H/(h + H)
      b_i = 1.0 - a_i
      A_dia[i], A_sup[i], A_sub[i] = 2., a_i, b_i
    A_sub[N - 1], A_dia[N - 1], b[N - 1] = b_n, 2.0, d_n
    self.C = linear_algebra.solve_tridiagonal_system(N, A_sub, A_dia, A_sup, b)

  def __call__(self, x):
    xs, ys, C = self.abscissae, self.ordinates, self.C
    i, j, _, _, _, _ = interpolation_base.locate(self, x)
    h_i = xs[j] - xs[i]
    x_low = xs[j] - x
    x_low3 = math.pow(x_low, 3)
    x_high = x - xs[i]
    x_high3 = math.pow(x_high, 3)
    hi_sqrd_6 = h_i*h_i/6.0

    return  C[i]*x_low3/(6.0*h_i)+C[j]*x_high3/(6.0*h_i)+\
           (ys[i]-C[i]*hi_sqrd_6)*x_low/h_i+(ys[j]-C[j]*hi_sqrd_6)*x_high/h_i
\end{verbatim}

\section{Root-finding}

We will present two schemes in this section for finding the roots of a
function $y = f(x)$ with $f: \mathbb R \mapsto \mathbb R$.

\subsection{Bisection Method}

The bisection method is a classiscal root-finding routine that does not
require derivative information. The \verb|ppf.math.root_finding|
module provides the following implementation derived from the
Boost.Math\_Toolkit library:

\begin{verbatim}
import math
from special_functions import sign, max_flt

def bisect(f, min, max, tol, max_its):
  """Bisection method
  """

  fmin, fmax = f(min), f(max)
  if fmin == 0: return (min, min, 0)
  if fmax == 0: return (max, max, 0)
  if min >= max: raise RuntimeError, "Arguments in wrong order"
  if fmin*fmax >= 0: raise RuntimeError,   "Root not bracketed"

  count = max_its
  if count < 3:
    count = 0
  else: count -= 3

  while count and tol(min, max) == 0:
    mid = (min + max)/2.
    fmid = f(mid)
    if mid == max or mid ==min:
        break
    if fmid == 0:
      min = max = mid
      break
    elif sign(fmid)*sign(fmin) < 0:
      max, fmax = mid, fmid
    else:
      min, fmin = mid, fmid
    --count

  max_its -= count

  return (min, max, max_its)
\end{verbatim}
The quadratic $y(x) = x^2 + 2x -1$ has roots
\begin{eqnarray*}
-1 - \sqrt{2} & = & -2.4142135623730950488016887242097\\
-1 + \sqrt{2} & = & +0.4142135623730950488016887242097.
\end{eqnarray*}
In the example interactive session, we find those roots by bisection:
\begin{verbatim}
>>> bisect(lambda x: x*x + 2*x - 1, -3, -2, 
... lambda x, y: math.fabs(x-y) < 0.000001, 10)
(-2.4142141342163086, -2.4142131805419922, 3)

>>> bisect(lambda x: x*x + 2*x - 1, 0, 1, 
... lambda x, y: math.fabs(x-y) < 0.000001, 10)
(0.41421318054199219, 0.41421413421630859, 3)
\end{verbatim}

\subsection{Newton-Raphson Method}

Newton-Raphson is a root-finding routine using derivatives with a
faster rate of convergence than bisection. The
\verb|ppf.math.root_finding| module offers an implementation again
derived from a Boost.Math\_Toolkit implementation:
\begin{verbatim}
def newton_raphson(f, guess, min, max, digits, max_its):
  """Newton-Raphson method
  
  """

  def _handle_zero_derivative(f, last_f0, f0, delta, result, guess, min, max):
    if last_f0 == 0:
      # must be first iteration
      if result == min: guess = max
      else: guess = min
      last_f0, _ = f(guess)
      delta = guess - result
    if sign(last_f0)*sign(f0) < 0:
      # we've crossed over so move in opposite
      # direction to last step
      if delta < 0:
        delta = (result - min)/2.0
      else:
        delta = (result - max)/2.0
    else:
      # move in same direction of last step
      if delta < 0:
        delta = (result - max)/2.0
      else:
        delta = (result - min)/2.0
    return (last_f0, delta, result, guess)

  f0, f1, last_f0, result = 0.0, 0.0, 0.0, guess
  factor = math.ldexp(1.0, 1 - digits)
  delta, delta1, delta2 = 1.0, max_flt(), max_flt()
  count = max_its

  while True:
    last_f0 = f0
    delta2 = delta1
    delta1 = delta
    f0, f1 = f(result)
    if f0 == 0:
      break
    if f1 == 0:
      last_f0, delta, result, guess = \
               _handle_zero_derivative( \
                     f, last_f0, f0, delta, result, guess, min, max)
    else:
      delta = f0/f1

    if math.fabs(delta*2.0) > math.fabs(delta2):
      # last two steps haven't converged, try bisection
      delta = ((result - max)/2.0, (result - min)/2.0)[delta > 0]
    guess = result
    result -= delta
    if result <= min:
      delta = 0.5*(guess - min)
      result = guess - delta
      if result == min or result == max:
        break
    elif result >= max:
      delta = 0.5*(guess - max)
      result = guess - delta
      if result == min or result == max: break

    # update brackets
    if delta > 0:
      max = guess
    else:
      min = guess

    count -= 1

    if count != 0 and \
           math.fabs(result*factor) < math.fabs(delta):
      continue
    else:
      break

  max_its -= count

  return (result, max_its)
\end{verbatim}
We apply it in the following interactive session to once again compute
a root of the polynomial from the preceeding section:
\begin{verbatim}
  >>> newton_raphson(lambda x: (x*x+2*x-1,2*x+2),-3,-3,-2,22,100)
  (-2.4142135623730949, 5)
\end{verbatim}

\section{Linear Algebra}

The Python NumPy package contains a module that covers most of what is
required from linear algebra from a financial engineering
perspective. For the most part, this section provides some examples of
its use for problems common in financial engineering. This section
just touches on the capabilities of NumPy for linear algebra. The
interested reader is referred to the NumPy documentation for further
detail. Readers with an interest in the development of linear algebra
routines in Python are encouraged to consult Jaan Kiusalaas's
``Numerical Methods in Engineering with Python''\cite{book:KIUSALAAS}.

\subsection{Matrix Multiplication}
The ordinary matrix product.
\begin{verbatim}
>>> from numpy import *
>>> from numpy.linalg import inv
>>> A = array([ [1, 3, 2], [1, 0, 0], [2, 1, 1]]) 
>>> M = matrix(A)
>>> print M*M
[[8 5 4]
 [1 3 2]
 [5 7 5]]
\end{verbatim}
Note the use of the matrix construction function in the example
above. It is this that gives the interpretation of the operation as a
matrix product. Multiplying two arrays on the other hand, gives the
product element-wise e.g.
\begin{verbatim}
>>> print A*A
[[1 9 4]
 [1 0 0]
 [4 1 1]]
\end{verbatim}
Matrix multiplication can be performed on arrays by use of the
\verb|dot| function as illustrated below.
\begin{verbatim}
>>> print dot(A, A)
[[8 5 4]
 [1 3 2]
 [5 7 5]]
\end{verbatim}

\subsection{Matrix Inversion}
Find the inverse of a square non-singular matrix 
\begin{verbatim}
>>> from numpy import *
>>> from numpy.linalg import inv
>>> A = array([ [1, 3, 2], [1, 0, 0], [2, 1, 1]]) 
>>> print A
[[1 3 2]
 [1 0 0]
 [2 1 1]]
>>> A_inv = inv(A)
>>> print A_inv
[[ 0.  1.  0.]
 [ 1.  3. -2.]
 [-1. -5.  3.]]
>>> print dot(A_inv, A)
[[ 1.  0.  0.]
 [ 0.  1.  0.]
 [ 0.  0.  1.]]
\end{verbatim}

\subsection{Matrix pseudo inverse}
Find the pseudo inverse of a matrix
\begin{verbatim}
>>> from numpy import *
>>> from numpy.linalg import inv
>>> A = array([ [1, 3, 2], [1, 0, 0 ] ])
>>> b = array([1, 3])
>>> x = dot(pinv(A), b)
>>> print x
[ 3.         -0.46153846 -0.30769231]
\end{verbatim}

\subsection{Solving linear systems}
Solve the linear system $\bf{A}\bf{x} = \bf{B}$ .
\begin{verbatim}
>>> from numpy import *
>>> from numpy.linalg import solve
>>> A = array([ [1, 3, 2], [1, 0, 0], [2, 1, 1]]) 
>>> b = array([4, 5, 6])
>>> print solve(A, b)
[  5.   7. -11.]
\end{verbatim}

\subsection{Solving tridiagonal systems}
Efficiently solve the linear system $\bf{A}\bf{x} = \bf{b}$ where A is
tridiagonal. This implementation is from the
\verb|ppf.math.linear_algebra| module.
\begin{verbatim}
def solve_tridiagonal_system(N, a, b, c, r):
  """Efficiently solve a tridiagonal system.

  For example if,
  
   x +  y = 3
   y +  z = 5
   y + 2z = 8

  then, 

  A = 3x3
      [   1    1    0   
          0    1    1
          0    1    2 ]

  and r = [ 3, 5, 8 ]' for which the expected
  result is x = [1, 2, 3].

  >>> a, b, c = [None, 0, 1], [1, 1, 2], [1, 1, None]
  >>> r =[3, 5, 8]
  >>> print solve_tridiagonal_system(3, a, b, c, r)
  [ 1.  2.  3.]
  
  """
  u, gam = numpy.zeros(N), numpy.zeros(N)
  bet = b[0]
  if bet == 0.0:
    raise RuntimeError, "Solve diagonal system error"
  u[0] = r[0]/bet
  for j in range(1, N):
    gam[j] = c[j - 1]/bet
    bet = b[j]- a[j]*gam[j]
    if bet == 0.0:
      raise RuntimeError, "Solve diagonal system error"
    u[j] = (r[j] - a[j]*u[j - 1])/bet
  for j in range(N - 2, -1, -1):
    u[j] -= gam[j + 1]*u[j + 1]

  return u
\end{verbatim}

\subsection{Solving upper diagonal systems}
Efficiently solve the linear system $\bf{A}\bf{x} = \bf{b}$ where A is
upper diagonal. The implementation shown below is from the
\verb|ppf.math.linear_algebra| module.
\begin{verbatim}
def solve_upper_diagonal_system(a, b):
  """Efficiently solve an upper diagonal system.

  For example, if

    A = 3 x 3
        [  1.75   1.5   -2.5
           0     -0.5    0.65
           0      0      0.25 ]
  and

    b = [  0.5   -1      3.5],

  the expected result is x = [2.97142857  20.2  14].
  
  >>> from numpy import *
  >>> A = matrix(array(
  ... [[1.75, 1.5, -2.5],
  ... [0.0, -0.5, 0.65],
  ... [0.0, 0.0, 0.25]], float))
  >>> A
  matrix([[ 1.75,  1.5 , -2.5 ],
          [ 0.  , -0.5 ,  0.65],
          [ 0.  ,  0.  ,  0.25]])
  >>> b = array([0.5, -1.0, 3.5])
  >>> b
  array([ 0.5, -1. ,  3.5])
  >>> x = solve_upper_diagonal_system(A, b)
  >>> x = matrix(x).transpose() # column vector
  >>> x
  matrix([[  2.97142857],
          [ 20.2       ],
          [ 14.        ]])
  >>> A*x  #matrix vector product
  matrix([[ 0.5],
          [-1. ],
          [ 3.5]])

  """
  if len(a.shape) <> 2:
    raise RuntimeError, "Expected 'a' to be a matrix"
  if a.shape[0] <> a.shape[1]:
    raise RuntimeError, "Expected 'a' to be a square matrix"
  if len(b.shape) <> 1:
    raise RuntimeError, "Expected 'b' to be a column vector"
  if b.shape[0] <> a.shape[0]:
    raise RuntimeError, "Expected 'b' to be a column vector"
  N = a.shape[0]
  for i in range(N):
    if a[i, i] == 0.0:
      raise RuntimeError, "Singular upper diagonal matrix"
    for j in range(0, i):
      if a[i, j] <> 0.0: raise RuntimeError, "Matrix not upper diagonal"
      
  x = numpy.zeros(N)
  for i in range(N-1, -1, -1):
    tmp = 0.0
    for j in range(i+1, N):
      tmp += a[i, j]*x[j]
    x[i] = (b[i]-tmp)/a[i, i]

  return x
\end{verbatim}

\subsection{Singular Value Decomposition}

We show how to calculate the singular value decomposition \index{linear algebra!singular value
decomposition} of an $M$ x $N$ matrix $\bf{A}$, with $M \ge N$,
into the product of a $M$ x $N$ orthogonal matrix $\bf{U}$, an
$N$ x $N$ diagonal matrix $\bf{W}$ with positive or zero elements (the
singular values), and the transpose of an $N$ x $N$ orthogonal matrix
$\bf{V}$. The actual implementation of the singular value decomposition 
algorithm is from \verb|numpy.linalg.svd| and the following code snippet illustrates 
a sample call to \verb|svd|.

\begin{verbatim}
>>> from numpy import *
>>> from numpy.linalg import svd
>>> A = transpose(array([[1., 3., 5.],[2., 4., 6.]]))
>>> print A
[[ 1.  2.]
 [ 3.  4.]
 [ 5.  6.]]
>>> U, sig, V = svd(A)
>>> print U
[[-0.2298477   0.88346102  0.40824829]
 [-0.52474482  0.24078249 -0.81649658]
 [-0.81964194 -0.40189603  0.40824829]]
>>> print sig
[ 9.52551809  0.51430058]
>>> print V
[[-0.61962948 -0.78489445]
 [-0.78489445  0.61962948]]
>>> n = 2
>>> W[:n, :n] = diag(sig)
>>> print W
[[ 9.52551809  0.        ]
 [ 0.          0.51430058]
 [ 0.          0.        ]]
>>> dot(U, dot(W, V))
array([[ 1.,  2.],
       [ 3.,  4.],
       [ 5.,  6.]])
\end{verbatim}

Given a singular value decomposition of a matrix $\bf{A} = \bf{U}
\bf{W} \bf{V}$, we can easily solve the matrix equation $\bf{A} \bf{x}
= \bf{b}$ using back substitution. The following implementation is
from the \verb|ppf.math.linear_algebra| module.
\begin{verbatim}
def singular_value_decomposition_back_substitution(u, w, v, b):
  """Solve an upper diagonal system using svd.

  For example, if

    A = 3 x 3
        [  1.75   1.5   -2.5
           0     -0.5    0.65
           0      0      0.25 ]
  and

    b = [  0.5   -1      3.5],

  the expected result is x = [2.97142857  20.2  14].
  >>> from numpy import *
  >>> from numpy.linalg import svd
  >>> A = matrix(array(
  ... [[1.75, 1.5, -2.5],
  ... [0.0, -0.5, 0.65],
  ... [0.0, 0.0, 0.25]], float))
  >>> A
  matrix([[ 1.75,  1.5 , -2.5 ],
          [ 0.  , -0.5 ,  0.65],
          [ 0.  ,  0.  ,  0.25]])
  >>> b = array([0.5, -1.0, 3.5])
  >>> b
  array([ 0.5, -1. ,  3.5])
  >>> u, w, v = svd(A)
  >>> x = singular_value_decomposition_back_substitution(u, w, v, b)
  >>> x = matrix(x).transpose() # column vector
  >>> x
  matrix([[  2.97142857],
          [ 20.2       ],
          [ 14.        ]])
  """

  if len(u.shape) <> 2:
    raise RuntimeError, "Expected 'u' to be a matrix"
  if len(w.shape) <> 1:
    raise RuntimeError, "Expected 'w' to be a column vector"
  if len(v.shape) <> 2:
    raise RuntimeError, "Expected 'v' to be a matrix"
  if len(b.shape) <> 1:
    raise RuntimeError, "Expected 'b' to be a column vector"

  m = u.shape[0]
  n = u.shape[1]

  if w.shape[0] <> n:
    raise RuntimeError, "'w' column vector has incorrect size"
  if b.shape[0] <> m: 
    raise RuntimeError, "'b' column vector has incorrect size"
  if v.shape[0] <> n or v.shape[1] <> n:
    raise RuntimeError, "'v' matrix has incorrect size"

  tmp = numpy.zeros(n)
  for j in range(n):
    s = 0.0
    if w[j] <> 0:
      for i in range(m):
        s += u[i, j]*b[i]
      s /= w[j]
    tmp[j] = s
  x = numpy.zeros(n)
  for j in range(n):
    s = 0.0
    for jj in range(n):
      s += v[jj, j]*tmp[jj]
    x[j] = s
  return x  
\end{verbatim}

\section{Generalised Linear Least Squares}
Generalised linear least squares is a method for fitting a set of data
points $(x_i, y_i)_{i=1,..,N}$ to a linear combination of basis
functions. The general form of this kind of model is
\begin{equation}
y(x) = \sum_{k=1}^M a_k f_k(x)
\end{equation}
where $f_1(x),...,f_M(x)$ are the basis functions. The central idea
behind the method is to find the fitting coefficients $a_1,...,a_M$ by
minimising the merit function
\begin{equation}
\chi^2 = \sum_{i=1}^N \left[\frac{y_i-\sum_{k=1}^M a_k f_k(x_i)}{\sigma_i}\right]^2.
\end{equation}
The $\sigma_i$ represents the measurement error, or equivalently the
standard deviation, of the $i$th data point. In \cite{book:PRESS} it
is shown that the solution of the above equation can be calculated by
solving the normal equations
\begin{equation}
\sum_{j=1}^M \alpha_{kj} a_j = \beta_k
\end{equation}
where
\begin{equation}
\alpha_{kj} = \sum_{i=1}^N \frac{f_j(x_i)f_k(x_i)}{\sigma_i^2}
\end{equation}
and
\begin{equation}
\beta_k = \sum_{i=1}^N \frac{y_i f_k(x_i)}{\sigma_i^2}.
\end{equation}
The normal equations can be solved using $LU$ decomposition and
backsubstitution but the solution is susceptible to roundoff error. It
is common practice to use singular value decomposition to solve this
problem, and this is the route we have taken.

The implementation of the generalised linear least squares algorithm
can be found in the \verb|ppf.math.generalised_least_squares| module
and the details of the implementation are shown below.
\begin{verbatim}
def generalised_least_squares_fit(y, x, sig, fit_fos):
  tol = 1.0e-13

  if len(y.shape) <> 1:
    raise RuntimeError, "Expected 'y' to be a column vector"
  if len(x.shape) <> 2:
    raise RuntimeError, "Expected 'x' to be a matrix"
  if len(sig.shape) <> 1:
    raise RuntimeError, "Expected 'sig' to be a column vector"

  ndata = x.shape[0]
  ma = len(fit_fos)

  if sig.shape[0] <> ndata:
    raise RuntimeError, "'sig' column vector has incorrect size"
  if y.shape[0] <> ndata:
    raise RuntimeError, "'y' column vector has incorrect size"


  a = numpy.zeros(ma)

  if ndata == 0:
   return a
  else:
   b = numpy.zeros(ndata)
   cu = numpy.zeros([ndata, ma])

   for i in range(ndata):
     xi = x[i, :]
     tmp = 1.0/sig[i]
     for j in range(ma):
       cu[i, j] = fit_fos[j](xi)*tmp
     b[i] = y[i]*tmp

   u, w, v = numpy.linalg.svd(cu, 0)
   wmax = numpy.max(w)
   threshold = tol*wmax
   for j in range(ma):
     if w[j] < threshold:
       w[j] = 0.0
   a = singular_value_decomposition_back_substitution(u, w, v, b)
   return a
\end{verbatim}
In an interpreter session, the generalised least squares algorithm can
be invoked as follows.
\begin{verbatim}
  >>> class linear_fo:
  ...   def __call__(self, x):
  ...     return x[0]
  >>> class quadratic_fo:
  ...   def __call__(self, x):
  ...     return x[0]*x[0]
  >>> generator = random.Random(1234)
  >>> ndata = 100
  >>> sig = numpy.zeros(ndata)
  >>> sig.fill(1.0)
  >>> y = numpy.zeros(ndata)
  >>> x = numpy.zeros([ndata, 1])
  >>> a = 0.25
  >>> b = -0.1
  >>> for i in range(ndata):
  ...   v = generator.gauss(0, 1.0)
  ...   x[i, 0] = v
  ...   y[i] = a*v+b*v*v
  >>> fit_fos = []
  >>> fit_fos.append(linear_fo())
  >>> fit_fos.append(quadratic_fo())
  >>> coeffs = generalised_least_squares_fit(y, x, sig, fit_fos)
  >>> coeffs
  array([ 0.25, -0.1 ])
\end{verbatim}

\section{Quadratic and Cubic roots}
It is not uncommon in finance to want to find the real roots of either
a quadratic or cubic equation. The module
\verb|ppf.math.quadratic_roots| provides an implementation for finding
the real roots of a quadratic. The real roots of the quadratic
equation
\begin{equation}
ax^2+bx+c = 0, ~~~~~~~a,b,c \in \mathbb R
\end{equation}
exist provided $b^2-4ac \ge 0$ and are given by the expression
\begin{equation}
r_{\pm} = -\frac{b}{2a} \pm \frac{\sqrt{b^2-4ac}}{2a}.
\end{equation}
For numerical stability reasons one shouldn't use the above equation
to determine the real roots. Instead it is better to calculate the
roots via the relations below
\begin{eqnarray}
q &=& -\frac{1}{2}\left(b+\mbox{sgn}(b)\sqrt{b^2-4ac}\right) \\
r_{+} &=& \frac{q}{a} \\
r_{-} &=& \frac{c}{q}
\end{eqnarray}
and this is precisely the way the real roots are calculated in the
\verb|ppf.math.quadratic_roots| module, as can been seen below.
\begin{verbatim}
import math
def quadratic_roots(a, b, c, xl, xh):
  # find roots
  roots = []
  d = b*b-4*a*c
  if d > 0:
    r1 = 0
    r2 = 0
    if a <> 0:
      sgn = 1
      if b < 0: sgn = -1
      q = -0.5*(b+sgn*math.sqrt(d))
      r1 = q/a
      r2 = r1
      if q <> 0: r2 = c/q
    else:
      r1 = -c/b
      r2 = r1
    # order roots
    if r1 > r2:
      tmp = r1
      r1 = r2
      r2 = tmp
    if r1 >= xl and r1 <= xh:
      roots.append(r1)
    if r2 <> r1 and r2 >= xl and r2 <= xh:
      roots.append(r2)
  else:
    if a <> 0:
      r1 = -b/(2*a)
      if r1 >= xl and r1 <= xh:
        roots.append(r1)

  return roots
\end{verbatim}

The real roots of the cubic equation
\begin{equation}
ax^3+bx^2+cx+d = 0, ~~~~~~~a,b,c,d \in \mathbb R
\end{equation}
are marginally more difficult to compute. The module
\verb|ppf.math.cubic_roots| contains an implementation of the cubic
roots algorithm. If $a \ne 0$ we proceed as follows. First we set $b$,
$c$ and $d$ to $\frac{b}{a}$, $\frac{c}{a}$ and $\frac{d}{a}$
respectively. Then we compute the variables $q$ and $r$ defined below
\begin{eqnarray}
q &:=& \frac{a^2-3b}{9} \\
r &:=& \frac{2a^3-9ab+27}{54}
\end{eqnarray}
For the case when $r^2 \le q^3$ the three real roots are given by
\begin{eqnarray}
r_1 &=& -2\sqrt{q} \cos\left(\frac{\theta}{3}\right)-\frac{a}{3} \\
r_2 &=& -2\sqrt{q} \cos\left(\frac{\theta+2\pi}{3}\right)-\frac{a}{3} \\
r_3 &=& -2\sqrt{q} \cos\left(\frac{\theta-2\pi}{3}\right)-\frac{a}{3}
\end{eqnarray}
with 
\begin{equation}
\theta := \arccos\left(\frac{r}{\sqrt{q}}\right)
\end{equation}
Otherwise there is only one real root given by
\begin{equation}
r = \frac{A+B}{3}
\end{equation}
with 
\begin{eqnarray}
A &=& -\mbox{sgn}(r) \left(\mbox{sgn}(r) r+\sqrt{r^2-q^3}\right)^{\frac{1}{3}} \\
B &=& \frac{q}{A}
\end{eqnarray}
Naturally, if $a$ is zero, then we fall back on the quadratic roots
algorithm already described. The implementation is summarised below.
\begin{verbatim}
import math
from quadratic_roots import *

def cubic_roots(a, b, c, d, xl, xh):
  if a <> 0:
    roots = []
    aa = a
    a = b/aa
    b = c/aa
    c = d/aa
    q = (a*a-3*b)/9.0
    r = (2*a*a*a-9.0*a*b+27.0*c)/54.0
    q3 = q*q*q
    diff = r*r-q3
    if diff <= 0:
      ratio = r/math.sqrt(q3)
      theta = math.acos(ratio)
      qr = -2.0*math.sqrt(q)
      a_over_3 = a/3.0
      r1 = qr*math.cos(theta/3.0)-a_over_3
      r2 = qr*math.cos((theta+2.0*math.pi)/3.0)-a_over_3
      r3 = qr*math.cos((theta-2.0*math.pi)/3.0)-a_over_3
      rs = [r1, r2, r3]
      rs.sort()
      [r1, r2, r3] = rs
      if r1 >= xl and r1 <= xh:
        roots.append(r1)
      if r2 <> r1 and r2 >= xl and r2 <= xh:
        roots.append(r2) 
      if r3 <> r1 and r3 <> r2 and r3 >= xl and r3 <= xh:
        roots.append(r3) 
    else:
      biga = 0  
      if r > 0:
        biga = -math.pow(r+math.sqrt(diff), 1.0/3.0)
      else:
        biga = math.pow(-r+math.sqrt(diff), 1.0/3.0)
      bigb = 0.0
      if biga <> 0: bigb = q/biga
      r1 = (biga+bigb)-a/3.0
      if r1 >= xl and r1 <= xh:
        roots.append(r1)
    return roots
  else:
    return quadratic_roots(b, c, d, xl, xh)
\end{verbatim}
Finally note that in the actual implementations we only return the
real roots, quadratic or cubic, if they lie in the range $\left[x_l,
x_h\right]$. Moreover we always sort the real roots. The reason for
doing this will become clear in the next section when we come to apply
the above algorithms in the context of integrating a polynomial.

\section{Integration}
In finance we often need to calculate the expectation of some function
$f: \mathbb R^n \mapsto \mathbb R$ of a number of random variables $X:
\Omega \mapsto \mathbb R^n$. Throughout this section we will only
consider financial payoffs that can be written in terms of a single
random variable $X: \Omega \mapsto \mathbb R$ and belong to the space
$C^3(\mathbb R)$, that is the space of continuous three-times
differentiable functions on $\mathbb R$. But the following can be
extended to higher dimensions with more effort.

\subsection{Piecewise constant polynomial fitting}
Let $X$ denote a random variable on $\mathbb R$ which we sample on a
uniform lattice $\{x_1, x_2, ...,x_N\}$ with spacing $\Delta$. The
corresponding values of the function $f$ on the uniform lattice are
denoted by $\{f_1, f_2, ...,f_N\}$. Similarly the first, second and
third derivatives of $f$ at any node of the lattice $x_i$ are denoted
by $f_i^{'}$, $f_i^{''}$ and $f_i^{'''}$ respectively. Our aim is to
fit the function $f$ piecewise on each interval $\left[x_i,
x_{i+1}\right]$ to the cubic polynomial shown below
\begin{equation}
f(x) = a_i+b_ix+c_ix^2+d_ix^3,~~~~\mbox{for } x \in \left[x_i, x_{i+1}\right] 
\end{equation} 
Taking the derivatives of the cubic polynomial we derive the following
upper diagonal matrix equation
\begin{eqnarray}
\left(\begin{array}{cccc}
1 & x_i & x_i^2 & x_i^3 \\
0 & 1 & 2 x_i & 3 x_i^2 \\
0 & 0 & 2 & 6 x_i \\
0 & 0 & 0 & 6
\end{array}\right) 
\left(\begin{array}{c}
a_i \\
b_i \\
c_i \\
d_i
\end{array}\right)  & = & 
\left(\begin{array}{c}
f_i \\
f_i^{'} \\
f_i^{''} \\
f_i^{'''}
\end{array}\right)
\end{eqnarray}
What remains is to derive expressions for the derivatives. If we wish
the fitted polynomial to be exact when the function $f$ happens to be
a cubic, then we have to be careful how we calculate the derivatives
numerically. One choice is given below
\begin{eqnarray}
f_i^{'} &=& \frac{\frac{1}{6} \left(f_{i-2}-f_{i+2}\right)+\frac{4}{3}\left(f_{i+1}-f_{i-1}\right)}{2 \Delta} \\
f_i^{''} &=& \frac{\left(f_{i+1}-2f_i+f_{i-1}\right)}{\Delta^2} \\
f_i^{'''} &=& \frac{\left(f_{i+2}-2f_{i+1}+2f_{i-1}-f_{i-2}\right)}{2 \Delta^3}
\end{eqnarray}
Note that the form of the derivatives means that we either have to
introduce ghost grids points at $\{x_{-1},x_0\}$ and
$\{x_{N+1},x_{N+2}\}$, or we only perform the fit on the sub-lattice
$\{x_2,x_3,...,x_{N-3},x_{N-2}\}$. The following implementation from 
the \verb|ppf.math.piecewise_polynomial_fitting| module performs the 
fitting on the sub-lattice and the solution of the upper diagonal system of 
linear equations is carried out by the \verb|solve_upper_diagonal_system|
function to be found in the \verb|ppf.math.linear_algebra| module.
\begin{verbatim}
def piecewise_cubic_fit(x, y):

  if len(x.shape) <> len(y.shape) and len(x.shape) <> 1:
    raise RuntimeError, "Mismatching 'x' and 'y' vectors"
  if x.shape[0] <> y.shape[0]:
    raise RuntimeError, "Mismatching 'x' and 'y' vectors"
  N = x.shape[0]
  if N < 4:
    raise RuntimeError, "Need at least 4 points"

  # assume uniform spacing
  one_sixth = 1.0/6.0
  four_thirds = 4.0/3.0
  dx_inv = 1.0/(x[1]-x[0])
  dx_inv2 = dx_inv*dx_inv
  dx_inv3 = dx_inv2*dx_inv
  coeffs = numpy.zeros((4, N - 4))

  a = numpy.zeros((4, 4))
  b = numpy.zeros(4)

  for i in range(2, N-2):
    # value
    b[0] = y[i]
    # first derivative
    b[1] = (one_sixth*(-y[i+2]+y[i-2])+four_thirds*(y[i+1]-y[i-1]))*0.5*dx_inv
    # second derivative
    b[2] = (y[i+1]-2.0*y[i]+y[i-1])*dx_inv2
    # third derivative
    b[3] = (y[i+2]-2.0*(y[i+1]-y[i-1])-y[i-2])*0.5*dx_inv3

    # fit matrix
    xi = x[i]
    xi2 = xi*xi
    xi3 = xi2*xi
    a[0][0] = 1.0
    a[0][1] = xi
    a[0][2] = xi2
    a[0][3] = xi3
    a[1][1] = 1.0
    a[1][2] = 2.0*xi
    a[1][3] = 3.0*xi2
    a[2][2] = 2.0
    a[2][3] = 6.0*xi
    a[3][3] = 6.0

    tmp = solve_upper_diagonal_system(a, b)
    for j in range(0, 4):
      coeffs[j, i-2] = tmp[j] 

  return coeffs
\end{verbatim}

\subsection{Piecewise polynomial integration}
Every random variable induces a distribution on $\mathbb R^n$. In this
section we assume the distribution induced by the random variable $X$
is normal. In the case of the normal distribution, the distribution is
fully specified by two parameters: the mean $\mu$ and the volatility
$\sigma$. Suppose we have a polynomial representation of our function
$f$ on the interval $[x_l, x_h]$, then the expectation of $f$ restricted to this
interval is given by
\begin{eqnarray}
\int_{x_l}^{x_h} f(x) n(x) dx &=&  \sum_{j=0}^m c_j \int_{x_l}^{x_h} x^j n(x) dx \\
  &=& \sum_{j=0}^m c_j \left( \int_{-\infty}^{x_r} x^j n(x) dx - \int_{-\infty}^{x_l} x^j n(x) dx \right)\\
  &=& \sum_{j=0}^m c_j \left(M_j(x_r)-M_j(x_l) \right)
\end{eqnarray}
with $M_j(y) := \int_{-\infty}^y x^j n(x) dx$ and $n(x)$ denotes the
normal probability distribution function. The partial moments can be easily
computed via the following recursion relationship:
\begin{eqnarray}
M_0(y) &=:& N(y) \\
M_1(y) &=& -\sigma^2 n(y)+\mu M_0(y) \\
M_m(y) &=& \sigma^2 \left(-y^{m-1} n(y)+(m-1) M_{m-2}\right)+\mu M_{m-1}  
\end{eqnarray}
with 
\begin{eqnarray}
n(y) &=& \frac{1}{\sqrt{2 \pi} \sigma} \exp(-\frac{1}{2} \left(\frac{x-\mu}{\sigma}\right)^2) \\
N(y) &=& \frac{1}{\sqrt{2 \pi} \sigma} \int_0^y \exp(-\frac{1}{2} \left(\frac{x-\mu}{\sigma}\right)^2) dx
\end{eqnarray}

The \verb|ppf.math.normal_distribution| module contains an implementation of
the integration of a piecewise polynomial function in a random variable 
with normal distribution. The method \verb|integral| on the \verb|normal_distribution| 
class carries out the integration.
\begin{verbatim}
class normal_distribution:
  def __init__(self, mean=0.0, vol=1.0):
    self.mean = mean
    self.vol = vol
    if vol < 0.0:
      raise RuntimeError, 'negative volatility'
    if vol <> 0.0:
      self.vol_inv = 1.0/vol
    else:
      self.vol_inv = 1.0;
    self.unit_norm = 1.0/math.sqrt(2.0*math.pi)

  def unit_pdf(self, x):
    return math.exp(-0.5*x*x)*self.unit_norm
       
  def pdf(self, x):
    y = x*self.vol_inv
    return self.unit_pdf(y)*self.vol_inv
 
  def unit_cdf(self, x):
    return N(x)
    
  def cdf(self, x):
    y = (x-self.mean)*self.vol_inv
    return self.unit_cdf(y)

  def moments(self, n, x):
    ys = (n)*[0.0]
    ys[0] = self.cdf(x)
    if n > 1:
      vol2 = self.vol*self.vol
      pdfx = self.pdf(x)
      ys[1] = -vol2*pdfx+self.mean*ys[0]
      xn = x
      for i in range(2, n):
         ys[i] = vol2*(-xn*pdfx+(i-1)*ys[i-2])+self.mean*ys[i-1]
         xn = xn*x
    return ys
        
  def integral(self, cs, xl, xh, yls = None, yhs = None):
    if xl > xh:
      raise RuntimeError, \
            'lower bound greater than upper bound of integration domain'
    n = len(cs)
    if yls == None:
      yls = self.moments(n, xl)
    else:
      if len(yls) <> n:
        raise RuntimeError, \
              "number of moments doesn't match number of coefficients"
    if yhs == None:
      yhs = self.moments(n, xh)
    else:
      if len(yhs) <> n:
        raise RuntimeError, \
              "number of moments doesn't match number of coefficients"
    sum = 0.0
    for i in range(n):
      sum += cs[i]*(yhs[i]-yls[i])
    return sum

  def state(self, stddev, n):
    if n < 2:
      raise RuntimeError, 'number of points must be greater than one'
    s = numpy.zeros(n)
    dx = 2*stddev/(n-1)
    for i in range(n):
       s[i] = self.mean+self.vol*(-stddev+i*dx)
    return s
\end{verbatim}

Now suppose we wish to calculate $\int_{x_l}^{x_h} \mbox{max}(f(x), 0) n(x) dx$. 
We can rewrite this integral as $\int_{x_l}^{x_h} f(x) \mathbbm{1}_{f(x) > 0}  n(x) dx$. 
In other words the original integral is a specialisation of the more general integral 
$\int_{x_l}^{x_h} f(x) \mathbbm{1}_{g(x) > 0}  n(x) dx$. To calculate the
more general integral we need to find the critical point $x^*$ at which
$g(x^*)=0$. Assuming we have a piecewise polynomial representation of
$g$, then the algorithm for finding the critical root is trivial. All we
need to do is delegate to the \verb|cubic_roots| function in the module
\verb|ppf.math.cubic_roots| to see if there are any real roots in the
interval. The method \verb|__bounds_| on the \verb|normal_distribution| class 
determines the corresponding sub-intervals from the roots.
\begin{verbatim}
class normal_distribution:
  def __bounds_(self, cs, xl, xh):
    # cubic coefficients
    n = len(cs)
    a = 0
    b = 0
    c = 0
    d = 0
    if n >= 1:
      d = cs[0]
    if n >= 2:
      c = cs[1]
    if n >= 3:
      b = cs[2]
    if n == 4:
      a = cs[3]
    if n > 4:
      raise RuntimeError, 'can only handle up to cubics'
    # roots
    roots = cubic_roots(a, b, c, d, xl, xh)
    bounds = []
    # calculate bounds
    xprev = xl
    for root in roots:
      xcurr = root
      xmid = 0.5*(xprev+xcurr)
      if d+xmid*(c+xmid*(b+xmid*a)) > 0:
        bounds.append([xprev, xcurr])
      xprev = xcurr
    xcurr = xh      
    xmid = 0.5*(xprev+xcurr)
    if d+xmid*(c+xmid*(b+xmid*a)) > 0:
      bounds.append([xprev, xcurr])
    return bounds
\end{verbatim}
Note that if there are real roots, then we loop through each of the
roots and only add a sub-interval if the function at the mid-point is positive. 
The actual integration then reduces to a loop over the bounds and is implemented in 
the method \verb|integral_indicator|.
\begin{verbatim}
class normal_distribution:
  def integral_indicator(self, cs, indicator, xl, xh, yls = None, yhs = None):
    '''
    >>> mean = 0.05
    >>> vol = 0.1
    >>> f = normal_distribution(mean, vol)
    >>> yls = f.moments(4, -1)
    >>> yhs = f.moments(4, 10000)
    >>> cs = [0.0,0.0,1.0,1.0]
    >>> print f.integral_indicator(cs, cs, -10000, 10000)
    0.014125
    >>> print cs[2]*(yhs[2]-yls[2])+cs[3]*(yhs[3]-yls[3])
    0.014125
    '''
    if xl > xh:
      raise RuntimeError, \
           'lower bound greater than upper bound of integration domain'
        
    bounds = self.__bounds_(indicator, xl, xh)
    sum = 0
    for bound in bounds:
       xll = bound[0]
       xrr = bound[1]
       yll = None
       yrr = None
       if xll == xl:
         yll = yls
       if xrr == xh:
         yrr = yhs
       sum += self.integral(cs, xll, xrr, yll, yrr)
    return sum
\end{verbatim}
We will discover later that it is often useful to regrid a piecewise
polynomial representation onto another grid. The regridding algorithm
is simple. Suppose we have a piecewise polynomial representation of
$f$ on the grid $\{x_2,x_3,...,x_{N-3},x_{N-2}\}$ and wish to regrid
the function onto the grid $\{y_1, y_2, ..., y_{N-1}, y_N\}$. Then all
we have to do is loop through each of the $y_i$'s and determine the
closest interval $\left[x_j, x_{j+1}\right]$ using the
\verb|equal_range| function from the \verb|ppf.utility.bound| module.
Once we have the closest interval we reconstruct the function at $y_i$
by using the polynomial representation corresponding to the closest
interval. The \verb|regrid| method on the \verb|normal_distribution| class 
is an implementation of the regridding algorithm.
\begin{verbatim}
class normal_distribution:
  def regrid(self, xs, cs, regrid_xs):
    m = len(xs)
    n = len(regrid_xs)
    # regrid function
    regrid_fs = numpy.zeros(n)
    for i in range(n):
      x = regrid_xs[i]
      # bound
      left, right = ppf.utility.equal_range(x, xs)
      if right == m: right -= 1
      if left == right: left -= 1
      idx = left
      # saturate
      if idx < 2: idx = 2
      if idx > m-3: idx = m-3
      csi = cs[:, idx-2]
      regrid_fs[i] = csi[0]+x*(csi[1]+x*(csi[2]+x*csi[3]))
    return regrid_fs 
\end{verbatim}
For completeness we also provide the method \verb|integral_max| on the 
\verb|normal_distribution| class. As expected, the calculation of the 
integral is performed by \verb|integral_indicator|.
\begin{verbatim}    
class normal_distribution:
  def integral_max(self, cs, xl, xh, yls = None, yhs = None):
    '''
    >>> mean = 0.05
    >>> vol = 0.1
    >>> f = normal_distribution(mean, vol)
    >>> yls = f.moments(4, -1)
    >>> yhs = f.moments(4, 10000)
    >>> cs = [0.0,0.0,1.0,1.0]
    >>> print f.integral_max(cs, -10000, 10000)
    0.014125
    >>> print cs[2]*(yhs[2]-yls[2])+cs[3]*(yhs[3]-yls[3])
    0.014125
    '''
    if xl > xh:
      raise RuntimeError, \
           'lower bound greater than upper bound of integration domain'
        
    return self.integral_indicator(cs, cs, xl, xh, yls, yhs)
\end{verbatim}

For reasons of speed optimisation we allow the client of the
\verb|normal_distribution| to supply precomputed moments at the
integral bounds. By default, if the moments are not supplied, then the
moments get calculated on the fly.

We conclude the section with a few sample calls onto the methods of
the normal distribution class. First of all we check that if the
initial grid used in the fitting is identical to the regridding grid,
then the regridded function should be the same as the original
function. The following code snippet confirms this.
\begin{verbatim}
>>> mean = 0.05
>>> vol = 0.1
>>> f = normal_distribution(mean, vol)
>>> xs = f.state(4.5, 10)
>>> cs = numpy.zeros((4,6))
>>> for i in range(6): cs[:, i] = 1.0, 0.0, 1.0, 0.0
>>> regrid_fs = f.regrid(xs, cs, xs)
>>> for i in range(2, 8): print "%f, %f" % (regrid_fs[i]\
      , cs[0,i-2]+xs[i]*(cs[1,i-2]+xs[i]*(cs[2,i-2]+xs[i]*cs[3,i-2]))) 
1.040000, 1.040000
1.010000, 1.010000
1.000000, 1.000000
1.010000, 1.010000
1.040000, 1.040000
1.090000, 1.090000
\end{verbatim}

The next code snippet checks various limits of the \verb|integral|
method. First of all we check that the expectation of one is also
one. Second we check that the expectation of a normal variable $X$
with mean $0.05$ is equal to the mean. Finally we check the
expectation of the square of the normal variable with mean $0.05$ and
volatility $0.1$ is equal to the sum of the mean and the volatility
squared.
\begin{verbatim}
>>> mean = 0.05
>>> vol = 0.1
>>> f = normal_distribution(mean, vol)
>>> cs = [1.0]
>>> print f.integral(cs, -10000, 10000)
1.0
>>> cs = [0.0,1.0]
>>> print f.integral(cs, -10000, 10000)
0.05
>>> cs = [0.0,0.0,1.0]
>>> print f.integral(cs, -10000, 10000)
0.0125
\end{verbatim}

The final code snippet verifies that the expectation of
$\mbox{max}(X^2+X^3,0)$ is equivalent to $\int_{-1}^{\infty}
\left(x^2+x^3\right)n(x)dx$.
\begin{verbatim}
>>> mean = 0.05
>>> vol = 0.1
>>> f = normal_distribution(mean, vol)
>>> yls = f.moments(4, -1)
>>> yhs = f.moments(4, 10000)
>>> cs = [0.0,0.0,1.0,1.0]
>>> print f.integral_max(cs, -10000, 10000)
0.014125
>>> print cs[2]*(yhs[2]-yls[2])+cs[3]*(yhs[3]-yls[3])
0.014125
\end{verbatim}

\subsection{Semi-analytic conditional expectations}
In the previous subsection we discussed how we can calculate the
integral of a function of a normally distributed random variable by
fitting the function piecewise to a polynomial. In this subsection we
discuss how to use this integration scheme to compute conditional
expectations.  Throughout this section we denote the function of the
one-dimensional Brownian motion\footnote{In 1828 the Scottish botanist 
Robert Brown observed that pollen grains suspended in liquid performed 
an irregular motion - Brownian motion.} $X_T$ by $y_T:=f(X_T)$ and the
filtration at time $t$ by $\mathcal F_t$.  A full mathematical definition 
of Brownian motion can be found in \cite{book:OEKSENDAL} but for the purposes 
of what follows all we need to know is that $X_T-X_t$ is independent of $X_t$ for $t < T$ 
and is normally distributed with zero mean and variance $T-t$. The aim of this
subsection is to explain how to compute the following conditional
expectations:
\begin{eqnarray}
y_t &:=& \mathbb E[y_T | \mathcal F_t] \\
y^+_t &:=& \mathbb E[\mbox{max}(y_T, 0) | \mathcal F_t]
\end{eqnarray}
Because the increments of a Brownian motion are independent of each
other the above conditional expectations can be written as
\begin{eqnarray}
y_t(x) &=& \mathbb E[f(x+X_T-X_t) | x_t = x] \\ 
y^+_t(x) &=& \mathbb E[\mbox{max}(f(x+X_T-X_t), 0) | x_t = x]
\end{eqnarray}
We donote the discrete grid of states for the Brownian motion
increment $X_T-X_t$ by $x_{tT}$. If we fit the function $f$ on the
discrete grid of $X_T$, denoted by $x_T$, then $x+x_{tT}$ for $X_t =
x$ will generally lie outside the domain of the discrete grid
$x_T$. Therefore we are forced to regrid the function $f$. Fortunately
we already know how to do this if the function $f$ is fitted piecewise
to a polynomial. In summary, the conditional expectations at $X_t = x$
are calculated in three steps: (a) given the piecewise polynomial fit
of $f$ on $X_T$ regrid the function onto the grid $x+x_{tT}$; (b) fit
the regridded $f$ to a piecewise polynomial on the grid $x_{tT}$; and
(c) use the integration schemes of the previous subsection to
calculate the integrals. Obviously in a standard application of the
algorithm, the steps are repeated for a discrete grid of $X_t$,
denoted $x_t$.

The code for computing the conditional expectations can be found in
the module \verb|ppf.math.semi_analytic_domain_integrator| and is
detailed below. The actual implementation of the conditional integrals is 
done by the private method \verb|__rollback_|. To make this possible we 
are forced to pass through the regridder and integrator functions. In the 
case of the \verb|rollback| method the integrator is simply the \verb|integral| 
member function of the distribution class represented by \verb|ftT|, and 
in the case of the \verb|rollback_max| method the integrator is the 
\verb|integral_max| member function of \verb|ftT|.
\begin{verbatim}
class semi_analytic_domain_integrator:
    
  def __create_cached_moments(self, x, f):
    n = x.shape[0]
    self.__ys = numpy.zeros([n, 4])
    self.__ys[2] = f.moments(4, x[2]) # cubic
    for j in range(2, n-2):
      self.__ys[j+1] = f.moments(4, x[j+1]) # cubic

  def __rollback_(self, t, T, xt, xT, xtT, yT, regridder, integrator):
    if len(xt.shape) <> len(xT.shape) or \
       len(xT.shape) <> len(yT.shape) or \
       len(xt.shape) <> 1 or len(xtT.shape) <> 1:
      raise RuntimeError, 'expected one dimensional arrays'

    nt = xt.shape[0]
    nT = xT.shape[0]
    ntT = xtT.shape[0]

    if nt <> nT or ntT <> nT:
      raise RuntimeError, 'expected array to be of same size'

    if yT.shape[0] <> nT:
      raise RuntimeError, \
           'array yT has different number of points to xT'

    yt = numpy.zeros(nt)
    cT = piecewise_cubic_fit(xT, yT)
    for i in range(nt):
      # regrid
      regrid_xT = numpy.zeros(nT)
      xti = xt[i]
      for j in range(nT):
        regrid_xT[j] = xti+xtT[j]
      regrid_yT = regridder(xT, cT, regrid_xT)
      # polynomial fit
      cs = piecewise_cubic_fit(xtT, regrid_yT)
      # perform expectation
      sum = 0
      xl = xtT[2]
      for j in range(2, nT-2): # somehow this should be enscapsulated
        xh = xtT[j+1]
        sum = sum + integrator(cs[:, j-2], xl, xh, self.__ys[j], self.__ys[j+1])
        xl = xh
      yt[i] = sum
      if t == 0.0:
        for j in range(1, nt):
          yt[j] = yt[0]
        break

    return yt
    
  def rollback(self, t, T, xt, xT, xtT, ftT, yT):
    # create cache of moments
    self.__create_cached_moments(xtT, ftT)    

    return self.__rollback_(t, T, xt, xT, xtT, yT, ftT.regrid, ftT.integral)

  def rollback_max(self, t, T, xt, xT, xtT, ftT, yT):
    # create cache of moments
    self.__create_cached_moments(xtT, ftT)    

    return self.__rollback_(t, T, xt, xT, xtT, yT, ftT.regrid, ftT.integral_max)
\end{verbatim}
Note that we precompute the moments in the function
\verb|__create_cached_moments()| prior to performing the
rollback. Doing this improves the efficiency of the algorithm
dramatically because otherwise we keep computing (unnecessarily) the
moments \verb|nt| times. Typically \verb|nt|$\approx 41$, so you can
see why we make such a huge computational saving by precomputing the
moments.

A number of tests have been written for the semi-analytic domain
integrator. The tests can be found in the module
\verb|ppf.test.test_math| with each separate test represented by a
method of the class \verb|integrator_tests|. The first test checks
that the we can perform the conditional expectation of the following
classical exponential martingale correctly\footnote{A martingale $M_t$
is a random variable satisfying the properties: $\mathbb E[\left| M_t
\right| ] < \infty$ for all $t$ and $\mathbb E[M_t | \mathcal F_t] =
M_s$ for $s \le t$}:
\begin{equation}
y_T = \exp\left(\sigma X_T - \frac{1}{2} \sigma^2 T\right)
\end{equation}
with $\sigma \in \mathbb R^{+}$. Because we know $y_T$ is a martingale
the following identity must hold.
\begin{equation}
\mathbb E[y_T | \mathcal F_t] = \exp\left(\sigma X_t -\frac{1}{2} \sigma^2 t\right)
\end{equation}
The snippet of code below gives the details of the test just
described.
\begin{verbatim}
  def lognormal_martingale_test(self):
    integrator = ppf.math.semi_analytic_domain_integrator()
    nt = 31
    nT = 31
    ntT = 31
    t = 0.5
    T = 1.0
    mut = 0.0
    muT = 0.0
    vol = 0.2
    volt = vol*math.sqrt(t)
    volT = vol*math.sqrt(T)
    ft = ppf.math.normal_distribution(mut, volt)
    fT = ppf.math.normal_distribution(muT, volT)
    xt = ft.state(5.5, nt)
    xT = fT.state(5.5, nT)
    meantT = muT-mut
    voltT = math.sqrt(volT*volT-volt*volt)
    ftT = ppf.math.normal_distribution(meantT, voltT)
    xtT = ftT.state(5.5, ntT)
    yT = numpy.zeros(nT)
    for i in range(nT): 
      yT[i] = math.exp(xT[i]-0.5*volT*volT) # lognormal martingale
    yt = integrator.rollback(t, T, xt, xT, xtT, ftT, yT)
    assert math.fabs(yt[15] - 0.990050) < 1.0e-6
\end{verbatim}
The second test verifies that the tower law holds, i.e. 
\begin{equation}
\mathbb E[y_T| \mathcal F_s] = \mathbb E[\mathbb E[y_T | \mathcal F_t]\mathcal F_s] \mbox{ for } s \le t \le T. 
\end{equation}
The following code snippet provides the details of the tower law test.
\begin{verbatim}
  def tower_law_test(self):
    integrator = ppf.math.semi_analytic_domain_integrator()
    nt = 31
    nT = 31
    ntT = 31
    t = 0.5
    T = 1.0
    mut = 0.0
    muT = 0.0
    vol = 0.2
    volt = vol*math.sqrt(t)
    volT = vol*math.sqrt(T)
    ft = ppf.math.normal_distribution(mut, volt)
    fT = ppf.math.normal_distribution(muT, volT)
    xt = ft.state(5.5, nt)
    xT = fT.state(5.5, nT)
    meantT = muT-mut
    voltT = math.sqrt(volT*volT-volt*volt)
    ftT = ppf.math.normal_distribution(meantT, voltT)
    xtT = ftT.state(5.5, ntT)
    yT = numpy.zeros(nT)
    for i in range(nT): 
      yT[i] = math.exp(xT[i]-0.5*volT*volT) # lognormal martingale
    yt = integrator.rollback(t, T, xt, xT, xtT, ftT, yT)
    ns = 31
    s = 0
    mus = 0.0
    vols = 0.0
    fs = ppf.math.normal_distribution(mus, vols)
    xs = fs.state(5.5, ns)
    meansT = muT-mus
    volsT = math.sqrt(volT*volT-vols*vols)
    fsT = ppf.math.normal_distribution(meansT, volsT)
    xsT = fsT.state(5.5, ntT)
    ys = integrator.rollback(s, T, xs, xT, xsT, fsT, yT)
    meanst = mut-mus
    volst = math.sqrt(volt*volt-vols*vols)
    fst = ppf.math.normal_distribution(meanst, volst)
    xst = fst.state(5.5, ntT)
    ys1 = integrator.rollback(s, t, xs, xt, xst, fst, yt)
    for i in range(ns):
      assert math.fabs(ys[i]-ys1[i]) < 1.0e-6
\end{verbatim}
Lastly the third test verifies that the value of at-the-money option
price satisfies the relation below
\begin{equation}
\mathbb E[\mbox{max}(y_T-1,0)] = 2 N(d_1)-1 
\end{equation}
with $N(x)$ the cumulative distribution function of the normal
distribution with zero mean and volatility $\sigma$ and
\begin{equation}
d_1 = \frac{\sigma \sqrt{T}}{2}
\end{equation}
The code snippet below provides the details of the at-the-money option
test.
\begin{verbatim}
  def atm_option_test(self):
    integrator = ppf.math.semi_analytic_domain_integrator()
    nT = 31
    t = 0.5
    T = 1.0
    mut = 0.0
    muT = 0.0
    vol = 0.2
    volt = vol*math.sqrt(t)
    volT = vol*math.sqrt(T)
    fT = ppf.math.normal_distribution(muT, volT)
    xT = fT.state(5.5, nT)
    yT = numpy.zeros(nT)
    for i in range(nT): 
      yT[i] = math.exp(xT[i]-0.5*volT*volT) # lognormal martingale
    ns = 31
    nsT = 31
    s = 0
    mus = 0.0
    vols = 0.0
    fs = ppf.math.normal_distribution(mus, vols)
    xs = fs.state(5.5, ns)
    meansT = muT-mus
    volsT = math.sqrt(volT*volT-vols*vols)
    fsT = ppf.math.normal_distribution(meansT, volsT)
    xsT = fsT.state(5.5, nsT)
    for i in range(nT): 
      yT[i] -= 1.0 # strike 1.0
    ys = integrator.rollback_max(s, T, xs, xT, xsT, fsT, yT)
    d1 = 0.5*volT
    for i in range(ns): 
      assert math.fabs(ys[i] - (2.0*fsT.unit_cdf(d1)-1.0)) < 1.0e-4
\end{verbatim}
